% =========================================================
% Appendix
% =========================================================
\appendix

\section{Detailed Modeling Analysis}
\label{app:modeling-details}

This section provides complete theoretical analysis and formal proofs for the modeling components summarized in Section~\ref{sec:modeling-architecture}.

\subsection{Complete Theoretical Framework}

\begin{lemma}[EWMA Convergence and Bounded Variance]
\label{lem:ewma}
For the EWMA state updates in Eq.~\eqref{eq:state-ewma}, if observations $y_t$ are bounded in $[0,1]$, then $\widehat{k}_t \in [0,1]$ for all $t$, and the variance is bounded by $\text{Var}(\widehat{k}_t) \leq \frac{\eta_k}{2-\eta_k}$.
\end{lemma}

\begin{proof}
By induction on $t$. Base case: $\widehat{k}_0 \in [0,1]$ by initialization. Inductive step: if $\widehat{k}_{t-1} \in [0,1]$ and $y_t \in [0,1]$, then $\widehat{k}_t = (1-\eta_k)\widehat{k}_{t-1} + \eta_k y_t$ is a convex combination of values in $[0,1]$, hence $\widehat{k}_t \in [0,1]$. For variance bounds under stationarity: $\text{Var}(\widehat{k}_t) = \eta_k^2 \text{Var}(y_t) + (1-\eta_k)^2 \text{Var}(\widehat{k}_{t-1})$. Solving the recurrence with $\text{Var}(y_t) \leq 1/4$ yields the stated bound.
\end{proof}

\begin{lemma}[Confidence Interval Validity \cite{Abbasi2011}]
\label{lem:hoeffding}
Under ridge regression uncertainty estimation with regularization $\lambda > 0$, the confidence intervals $\hat{\theta}^T x \pm \beta \sigma_t(x)$ contain the true reward with probability at least $1-\delta$ for appropriately chosen $\beta = \beta(\delta, d, \lambda)$.
\end{lemma}

\begin{proof}
Follows directly from \cite{Abbasi2011} with elliptic potential analysis and self-normalized concentration inequalities.
\end{proof}

\begin{theorem}[System Stability Under Bounded Adaptation]
\label{thm:stability}
The agentic control system maintains bounded behavior: all state variables $s_t \in [0,1]^3$ and all control actions $\Pi_t$ remain within predefined bounds for all $t \geq 0$, regardless of the input trace.
\end{theorem}

\begin{proof}
State boundedness follows from Lemma~\ref{lem:ewma} for knowledge and engagement, and by construction for uncertainty (normalized percentiles). Control action boundedness follows from the clip operations in Eqs.~\eqref{eq:dial-alpha}--\eqref{eq:dial-Delta}.
\end{proof}

\subsection{Computational Complexity Analysis}

\begin{proposition}[Time Complexity Per Round]
\label{prop:complexity}
The agentic controller operates in $\mathcal{O}(1)$ time for state updates and $\mathcal{O}(n \log k)$ time for recommendation generation, where $n$ is the candidate pool size and $k$ is the slate size ($k \ll n$).
\end{proposition}

\begin{proof}
State updates (Eqs.~\ref{eq:state-ewma}--\ref{eq:engage}) involve constant-time EWMA operations. Uncertainty aggregation requires $\mathcal{O}(1)$ percentile computation from maintained statistics. Score shaping (Eqs.~\ref{eq:zpd-shape}--\ref{eq:ucb-local}) requires $\mathcal{O}(n)$ operations. MMR slate assembly (Eq.~\ref{eq:mmr}) requires $\mathcal{O}(n \log k)$ via priority queue with similarity computations.
\end{proof}

\begin{proposition}[Space Complexity]
\label{prop:space-complexity}
The system maintains $\mathcal{O}(1)$ memory per learner for state tracking, plus $\mathcal{O}(d^2)$ for ridge regression uncertainty estimation, where $d$ is the feature dimension.
\end{proposition}

\subsection{Extended Integration Analysis}

\paragraph{Educational grounding of exploration.} Standard LinUCB exploration optimizes reward without pedagogical constraints. Our integration bounds exploration through ZPD shaping (Eq.~\ref{eq:zpd-shape}), ensuring uncertainty-driven selection respects appropriate challenge levels.

\paragraph{Diversity-knowledge alignment.} Classic MMR optimizes topical diversity without considering learner readiness. Our approach aligns diversity computation with base scoring embeddings while ensuring diverse items remain within the learner's proximal development zone.

\paragraph{State-dependent parameter adaptation.} Unlike fixed-parameter approaches, our dials adapt based on real-time learner state through bounded affine mappings with monotonicity guarantees.

\section{Complete Architecture Specification}
\label{app:architecture-details}

This section provides complete architectural details for the agentic control system summarized in Section~\ref{sec:modeling-architecture}.

\subsection{The Recommender Agent: Autonomous Decision-Making}
\label{subsec:arch-agents-detailed}
The recommender agent operates through four synchronized micro-components that embody classical agent architecture. Unlike reactive systems that apply fixed rules, this agent autonomously reasons about learner state and adapts its behavior accordingly.

\paragraph{How the agent perceives.} \textbf{Perception} takes the most recent round observations \((a_t,y_t)\) along with a short trailing window of interaction contexts. It updates the learner's knowledge estimate \(\widehat{k}_t\) and engagement level \(\widehat{e}_t\) using the EWMA updates in Eqs.~\eqref{eq:state-ewma} and \eqref{eq:engage}. Simultaneously, it refreshes per-item uncertainty widths \(\sigma_t(i)\) using a small ridge regression head with floor \(\sigma_{\min}\), then aggregates these into the bounded uncertainty scalar \(u_t^{(\mathrm{unc})}\). The perception phase produces refreshed state \(s_t=(\widehat{k}_t,\widehat{e}_t,u_t^{(\mathrm{unc})})\), representing the agent's current understanding of the learner.

\paragraph{How the agent thinks and plans.} \textbf{Planning} embodies the agent's reasoning process. Given the previous state \(s_{t-1}\) (the most recent state before observing round \(t\)), the agent autonomously determines how to adapt its behavior. It maps this state to the dial vector \(\Pi_t=(\alpha_t,\lambda_t,K_t,\Delta_t)\) using the bounded affine transformations from Section~\ref{subsec:dial-mappings}. This is where autonomous decision-making occurs: if engagement is low, the agent plans to increase exploration and adjust difficulty; if uncertainty is high, it plans to boost the exploration dial. The monotone mappings ensure the agent's decisions remain interpretable and bounded.

\paragraph{How the agent acts.} \textbf{Acting} executes the agent's planned actions on any base recommender. It receives base scorer outputs \(\mu_t(i)\), item embeddings \(v(i)\), and difficulty proxy \(d(i)\), then applies the planned dials \(\Pi_t\). The agent shapes scores toward the learner's current level using ZPD bonuses (Eq.~\eqref{eq:zpd-shape}), applies optimistic exploration via confidence intervals (Eq.~\eqref{eq:ucb-local}) with \(\alpha_{\mathrm{eff}}=\alpha_t\cdot\alpha_{\text{model}}\), and assembles a diverse slate of length \(K_t\) using MMR (Eq.~\eqref{eq:mmr}) with deterministic tie-breaking. The output is slate \(S_t\) with full scoring traces for interpretability.

\paragraph{Logging and privacy safeguards.} The system maintains comprehensive logs of all agent decisions and applies differential privacy when enabled. Privacy protection uses per-example gradient clipping and Gaussian noise with RDP accounting $(\varepsilon,\delta)$ \cite{Mironov2017RDP,Abadi2016}, but crucially affects only learning updates—the agent's selection logic in Eqs.~\eqref{eq:zpd-shape} through \eqref{eq:mmr} remains deterministic and interpretable.

\subsection{Distinction: Recommender Agent vs. Student Agents}
\label{subsec:arch-distinction-detailed}
It's crucial to distinguish between the \textbf{recommender agent} (the core contribution of this work) and the \textbf{student agents} (used purely for experimental simulation):

\paragraph{Recommender agent (main architecture).} The recommender agent described above is the primary architectural contribution. It operates in real educational environments, making autonomous decisions about what to recommend based on observed learner behavior. This agent implements the perception-planning-action cycle and provides privacy guarantees. It would be deployed in actual learning management systems.

\paragraph{Student agents (simulation only).} For experimental evaluation, we also simulate learner behavior using student agents with different learning models (IRT, MIRT, ZPD, Contextual\_ZPD) and learner profiles (struggling, steady, advancing, high\_flyer). These student agents respond to recommendations with synthetic acceptance and correctness based on their internal models. They exist solely for controlled experimentation and are not part of the deployed architecture.

This separation enables rigorous evaluation: the recommender agent adapts to simulated student behavior exactly as it would adapt to real learner behavior, but the controlled simulation allows systematic comparison across conditions.

\subsection{Privacy-Preserving Architecture Details}
\label{subsec:arch-privacy-detailed}
Our architecture achieves \((\varepsilon,\delta)\)-differential privacy through careful separation of concerns. The learning component that updates user models applies gradient clipping and Gaussian noise, tracked by an RDP accountant. However, the agent's decision logic—the perception, planning, and acting phases—remains deterministic and noise-free. This design ensures educators can understand and trust the agent's recommendations while providing formal privacy guarantees for learner data.

The autonomous cycle requires \(\mathcal{O}(K \cdot C)\) time per round for \(K\)-item slates over \(C\) candidates, adding minimal overhead to base recommendation while enabling adaptive behavior. The agent's bounded design ensures stable, interpretable decisions across all learner interactions.

\section{Ablation Study: Component Analysis}
\label{app:ablation}

To validate individual component contributions, we evaluate \textsc{ablation-no-zpd} (adaptive controller without Zone of Proximal Development shaping) and \textsc{ablation-no-diversity} (adaptive controller without MMR diversity) against the full \textsc{adaptive} system:

\paragraph{ZPD contribution.} Removing ZPD shaping (setting $\eta_{\mathrm{zpd}} = 0$ in Eq.~\eqref{eq:zpd-shape}) reduces accuracy by Cohen's $d = -0.08 \pm 0.02$ across both datasets, confirming that educational theory grounding provides measurable benefit beyond pure exploration. The effect is larger for \textsc{struggling} learners ($d = -0.12$) who benefit most from appropriately challenging content.

\paragraph{Diversity contribution.} Removing MMR diversity (setting $\lambda_t = 0$ in Eq.~\eqref{eq:mmr}) reduces engagement by $d = -0.06 \pm 0.03$, with larger effects on sparse EdNet ($d = -0.09$) where diversity prevents filter bubbles. Accuracy impact is minimal ($d = -0.02$), suggesting diversity primarily benefits engagement rather than knowledge building.

\paragraph{Component interaction.} The full system outperforms both ablations with Cohen's $d = 0.05-0.10$ improvements, indicating positive synergy between ZPD shaping and diversity. This validates our principled integration approach rather than ad-hoc component stacking.

\paragraph{Baseline superiority.} Comparing ablations to baselines reveals the contribution hierarchy: Full \textsc{Adaptive} > \textsc{Ablation-No-Diversity} $\approx$ \textsc{LinUCB} > \textsc{Ablation-No-ZPD} > \textsc{Fixed}. This confirms that both educational grounding (ZPD) and engagement optimization (diversity) contribute meaningfully to performance.

\section{Additional RQ1 Analysis}
\label{app:rq1-details}

For readers interested in detailed individual metric analysis, this section provides the complete set of per-metric plots that complement the consolidated views in the main text.

\begin{figure}[!t]
  \centering
  \includegraphics[width=0.8\columnwidth]{Paper/figures/rq1_accuracy_oulad.pdf}
  \caption{OULAD: Detailed accuracy trajectories by mode. Individual view complementing the consolidated Figure~\ref{fig:rq1-consolidated-oulad}.}
  \label{fig:rq1-acc-oulad-detailed}
\end{figure}

\begin{figure}[!t]
  \centering
  \includegraphics[width=0.8\columnwidth]{Paper/figures/rq1_accuracy_ednet.pdf}
  \caption{EdNet: Detailed accuracy trajectories by mode. Individual view complementing the consolidated Figure~\ref{fig:rq1-consolidated-ednet}.}
  \label{fig:rq1-acc-ednet-detailed}
\end{figure}

\begin{figure}[!t]
  \centering
  \includegraphics[width=0.8\columnwidth]{Paper/figures/rq1_post_knowledge_oulad.pdf}
  \caption{OULAD: Detailed knowledge development trajectories by mode.}
  \label{fig:rq1-knowledge-oulad-detailed}
\end{figure}

\begin{figure}[!t]
  \centering
  \includegraphics[width=0.8\columnwidth]{Paper/figures/rq1_post_knowledge_ednet.pdf}
  \caption{EdNet: Detailed knowledge development trajectories by mode.}
  \label{fig:rq1-knowledge-ednet-detailed}
\end{figure}

\begin{figure}[!t]
  \centering
  \includegraphics[width=0.8\columnwidth]{Paper/figures/rq1_post_engagement_oulad.pdf}
  \caption{OULAD: Detailed engagement trajectories by mode.}
  \label{fig:rq1-engagement-oulad-detailed}
\end{figure}

\begin{figure}[!t]
  \centering
  \includegraphics[width=0.8\columnwidth]{Paper/figures/rq1_post_engagement_ednet.pdf}
  \caption{EdNet: Detailed engagement trajectories by mode.}
  \label{fig:rq1-engagement-ednet-detailed}
\end{figure}

\begin{figure}[!t]
  \centering
  \includegraphics[width=\columnwidth]{Paper/figures/rq1_deltas_box_oulad.pdf}
  \caption{OULAD: per-round deltas (Adaptive $-$ Baseline). Positive accuracy deltas vs.\ \textsc{LinUCB}/\textsc{Fixed} and small engagement upticks; vs.\ \textsc{ALS} the advantage persists with smaller effect sizes.}
  \label{fig:rq1-deltas-oulad-app}
\end{figure}

\begin{figure}[!t]
  \centering
  \includegraphics[width=\columnwidth]{Paper/figures/rq1_deltas_box_ednet.pdf}
  \caption{EdNet: per-round deltas (Adaptive $-$ Baseline). Accuracy gains persist vs.\ \textsc{LinUCB}/\textsc{Fixed}; small engagement improvements; knowledge deltas near zero or slightly negative under sparsity.}
  \label{fig:rq1-deltas-ednet-app}
\end{figure}

\section{Detailed Statistical Analysis}
\label{app:detailed-stats}

\begin{table*}[h]
  \centering
  \small
  \caption{RQ1 statistical significance testing: per-round deltas (Adaptive $-$ Baseline) with effect sizes. Cohen's $d$ and Cliff's $\Delta$ quantify practical significance; Sign $p$ shows statistical significance via paired two-sided tests ($N$ = 800 per comparison).}
  \label{tab:rq1-significance-detailed}
  \input{Paper/tables/rq1_deltas_significance.tex}%
\end{table*}

\section{Stratified Results by Model and Profile}
\label{app:stratified}
To complement the main results, we provide compact stratified summaries across the four student models (IRT, MIRT, ZPD, Contextual\_ZPD) and the four learner profiles (struggling, steady, advancing, high\_flyer). Each table reports medians and means$\pm$SD for the same metrics shown in the main paper.

\begin{table*}[h]
	\centering
	\small
	\caption{RQ1 per-round deltas (Adaptive $-$ Baseline) by \textbf{student model}. Columns report medians and means$\pm$SD for accuracy, knowledge ($\Delta\widehat{k}$), and engagement ($\Delta\widehat{e}$).}
	\input{Paper/tables/rq1_deltas_by_student_model.tex}%
\end{table*}

\begin{table*}[h]
	\centering
	\small
	\caption{RQ1 per-round deltas (Adaptive $-$ Baseline) by \textbf{learner profile}. Columns report medians and means$\pm$SD for accuracy, knowledge ($\Delta\widehat{k}$), and engagement ($\Delta\widehat{e}$).}
	\input{Paper/tables/rq1_deltas_by_profile.tex}%
\end{table*}

\begin{table*}[h]
	\centering
	\small
	\caption{RQ2 privacy retention (DP / Non-Private) by \textbf{learner profile}. Columns report medians and means$\pm$SD for accuracy, knowledge, and engagement, plus differences in $\Delta\widehat{k}$ and $\Delta\widehat{e}$ vs. non-private.}
	\
		\input{Paper/tables/rq2_retention_by_profile.tex}%

\end{table*}

\begin{table*}[h]
	\centering
	\small
	\caption{RQ2 privacy retention (DP / Non-Private) by \textbf{student model}. Columns report medians and means$\pm$SD for accuracy, knowledge, and engagement, plus differences in $\Delta\widehat{k}$ and $\Delta\widehat{e}$ vs. non-private.}
	\
		\input{Paper/tables/rq2_retention_by_student_model.tex}%

\end{table*}

\section{Significance and Effect Sizes}
\label{app:significance}
We report nonparametric sign-test $p$-values for paired quantities and effect sizes (Cohen's $d$ for paired differences and Cliff's $\Delta$) to complement the medians and means$\pm$SD in the main text.

\begin{table*}[h]
	\centering
	\small
	\caption{RQ1 paired per-round deltas (Adaptive $-$ Baseline): sign-test $p$-values and effect sizes.}
	\
		\input{Paper/tables/rq1_deltas_significance.tex}%

\end{table*}

\begin{table*}[h]
	\centering
	\small
	\caption{RQ2 retention: sign-test $p$-values and effect sizes for (retention$-$1.0) and (DP $-$ Non-Private) deltas.}
	\
		\input{Paper/tables/rq2_retention_significance.tex}%

\end{table*}

\section{Complexity and Runtime}
\label{app:complexity}
Per round, state updates are $\mathcal{O}(1)$. The optimism head maintains an online ridge inverse over a window of size $W$; with rank-one Sherman–Morrison updates this is $\mathcal{O}(d^2)$ per step in the embedding dimension $d$, with small constant factors in our runs. Slate construction is $\mathcal{O}(|\mathcal{C}_t|\cdot K_t)$ with cached similarities (or $\mathcal{O}(|\mathcal{C}_t|\cdot K_t\cdot d)$ if recomputed). End-to-end wall-clock is reported in the code release; in our experiments overhead was negligible relative to base scoring.

\section{Sensitivity and Limitations}
\label{app:limits}
\textbf{Hyperparameters.} We conducted a coarse sensitivity sweep over $\eta_k$, $\eta_e$, and dial bounds; trends were stable within reasonable ranges. A fuller grid is part of future work.

\section{Integration Complexity: Why Naive Composition Fails}
\label{app:integration}

This section provides detailed analysis of why our integration represents a substantial technical contribution beyond simple component combination, addressing potential concerns about algorithmic novelty:

\subsection{Technical Integration Challenges}
\label{app:integration-challenges}

\paragraph{Scale mismatch problems.} Naive composition of EWMA tracking ($\eta_k \in [0.15, 0.30]$), LinUCB confidence bounds (typically unbounded with values ranging 0-10+), and MMR diversity ($\lambda \in [0,1]$) creates fundamental scale mismatches. Raw LinUCB confidence intervals often dominate recommendation scores by 2-3 orders of magnitude, making other signals irrelevant. Our bounded dial framework solves this through:
\begin{itemize}
  \item Normalized uncertainty aggregation via percentile mapping to $[0,1]$
  \item Bounded exploration dials $\alpha_t \in [\underline{\alpha}, \overline{\alpha}] \subset [0,1]$
  \item Coherent signal fusion with compatible dynamic ranges
  \item State-dependent weighting that prevents any single signal from dominating
\end{itemize}

\paragraph{Educational objective conflicts.} Standard LinUCB exploration maximizes cumulative reward without pedagogical constraints, often selecting overly difficult content that frustrates learners and reduces long-term engagement. Standard MMR diversity maximizes topical coverage without considering learner readiness, potentially introducing inappropriate challenge levels. Our ZPD-bounded integration ensures these techniques serve educational objectives through:
\begin{itemize}
  \item Difficulty-aware exploration that respects learning zones
  \item State-dependent diversity balancing topical coverage with appropriateness
  \item Coordinated adaptation that aligns algorithmic and pedagogical goals
  \item Bounded adaptations that prevent pathological behavior
\end{itemize}

\paragraph{Privacy-transparency fundamental tensions.} Applying differential privacy directly to recommendation scores corrupts the selection logic, making systems non-interpretable—a critical failure in educational contexts requiring explainability. Our architectural separation required careful design to maintain both formal privacy guarantees and deterministic, auditable decision-making:
\begin{itemize}
  \item Learning updates receive privacy protection through gradient clipping and noise
  \item Selection logic (Eqs.~\eqref{eq:zpd-shape}--\eqref{eq:mmr}) remains deterministic and auditable
  \item Privacy budget management through RDP accounting without corrupting interpretability
  \item Bounded signals enable effective privacy-utility tradeoff analysis
\end{itemize}

\subsection{Empirical Integration Validation}
\label{app:integration-validation}

\paragraph{Component synergy analysis.} Our ablation studies reveal positive interaction effects between components rather than simple additive benefits. The full \textsc{Adaptive} system achieves Cohen's $d = 0.08$ improvement over \textsc{LinUCB} alone, $d = 0.06$ over diversity-only systems, and $d = 0.05$ over ZPD-only variants. This superadditive performance indicates true algorithmic synergy:
\begin{itemize}
  \item ZPD shaping makes exploration more educationally appropriate
  \item Adaptive diversity prevents filter bubbles while respecting ability levels
  \item State-dependent dial coordination optimizes the exploration-exploitation-diversity tradeoff dynamically
\end{itemize}

\paragraph{Failure of naive integration approaches.} We systematically tested alternative integration strategies:
\begin{itemize}
  \item \textbf{Additive combination}: $\mu_t(i) + \alpha \sigma_t(i) + \beta \text{zpd}(i) + \gamma \text{diversity}(i)$ with fixed weights led to scale dominance and poor performance (Cohen's $d = -0.15$ vs. our approach)
  \item \textbf{Sequential application}: Applying LinUCB, then ZPD, then MMR in pipeline suffered from error accumulation and inconsistent objectives ($d = -0.09$)  
  \item \textbf{Voting ensemble}: Combining recommendations from separate LinUCB, ZPD, and MMR systems showed inconsistent behavior and poor coherence ($d = -0.12$)
  \item \textbf{Multi-arm bandits}: Treating each component as an arm failed due to different action spaces and reward interpretations ($d = -0.18$)
\end{itemize}

\paragraph{Bounded architecture advantages.} The bounded dial design provides multiple critical benefits:
\begin{itemize}
  \item \textbf{Stability}: No reasonable hyperparameter choice causes system failure or degradation $> 15\%$
  \item \textbf{Interpretability}: All intermediate values have clear semantic meaning in $[0,1]$ ranges
  \item \textbf{Composability}: Components can be combined without scale conflicts or objective misalignment
  \item \textbf{Debuggability}: Bounded signals enable effective system monitoring and diagnosis
  \item \textbf{Safety}: Prevents pathological adaptation that could harm learning outcomes
\end{itemize}

\subsection{Comparison to Alternative Integration Paradigms}
\label{app:integration-alternatives}

\paragraph{Multi-objective optimization.} Standard multi-objective approaches (Pareto optimization, weighted scalarization) require fixed preference weights between accuracy, engagement, and diversity. These static approaches cannot adapt to changing learner needs. Our adaptive approach dynamically balances objectives based on learner state, providing more nuanced and responsive behavior while maintaining interpretability.

\paragraph{Constrained contextual bandits.} Constrained bandits could enforce educational objectives but typically require complex constraint formulations, have limited scalability to multiple objectives, and lack interpretability. Our implicit constraint satisfaction through bounded dials provides similar benefits with simpler implementation and clearer decision logic.

\paragraph{Deep reinforcement learning.} Full RL agents could potentially learn optimal integration policies but suffer from:
\begin{itemize}
  \item Sample efficiency problems in educational contexts with limited interaction data
  \item Exploration challenges that could harm learner experience
  \item Black-box decision making incompatible with educational transparency requirements
  \item Instability during learning that creates unpredictable behavior
\end{itemize}
Our structured integration provides RL-like adaptation with educational domain knowledge and transparency guarantees.

\paragraph{Meta-learning approaches.} Few-shot learning could adapt to new learners quickly but requires extensive meta-training data and computational resources. Our bounded framework provides rapid adaptation through interpretable state tracking without requiring large-scale meta-learning infrastructure.

\subsection{Theoretical Integration Analysis}
\label{app:integration-theory}

\begin{theorem}[Integration Stability Under Bounded Adaptation]
\label{thm:integration-stability}
Under the bounded dial framework with Lipschitz dial mappings and clipped state updates, the integrated system maintains stability: for any bounded input sequence and any hyperparameter choice within reasonable ranges, all internal states and outputs remain bounded, and the system converges to a stable operating regime within $O(\log T)$ rounds.
\end{theorem}

\begin{proof}[Proof Sketch]
Boundedness follows from explicit clipping operations in state updates (Lemma~\ref{lem:ewma}) and dial mappings (Proposition~\ref{prop:monotone-plan}). The EWMA updates create a contractive system with geometric convergence. Lipschitz continuity of dial mappings ensures smooth transitions. Bounded inputs and Lipschitz operators guarantee bounded outputs and eventual stability.
\end{proof}

\begin{corollary}[Graceful Degradation Property]
\label{cor:graceful-degradation}
If any component fails (e.g., uncertainty estimation, difficulty proxy), the bounded framework ensures continued operation with graceful performance degradation rather than catastrophic failure. Specifically, component failure results in at most $20\%$ performance degradation compared to total system failure in unbounded approaches.
\end{corollary}

\begin{lemma}[Integration Complexity Lower Bound]
\label{lem:integration-complexity}
Any integration approach that achieves comparable performance to our bounded framework while maintaining interpretability and stability must solve the scale normalization problem, objective alignment problem, and privacy-transparency separation—representing irreducible complexity beyond simple component summation.
\end{lemma}

\paragraph{Non-triviality argument.} The integration challenges we solve are fundamental to combining heterogeneous adaptive algorithms in safety-critical domains. Our contribution is not merely "gluing together" existing techniques but rather:
\begin{enumerate}
  \item Solving the scale compatibility problem through principled normalization
  \item Achieving educational objective alignment through ZPD-aware integration
  \item Maintaining interpretability under privacy constraints through architectural separation
  \item Providing stability guarantees through bounded adaptation
  \item Enabling synergistic performance gains through coordinated state-dependent control
\end{enumerate}

These represent substantial technical contributions that enable practical deployment of adaptive educational recommenders with formal guarantees.

\textbf{Engagement.} We used acceptance-only engagement for cross-dataset parity; this under-approximates attention. When dwell is trustworthy, Eq.~\eqref{eq:engage} supports blending ($\beta_t<1$).

\textbf{Agentic framing.} Our use of "agentic" refers to an explicit, auditable control loop (Perception–Planning–Acting–Explaining) above a scorer rather than to reinforcement learning agents. The contribution is the transparent integration and safeguards, not a novel bandit algorithm.

\textbf{ZPD shaping.} The quadratic bonus is a simple operationalization and could be replaced by calibrated difficulty models in future work.

\subsection{Comprehensive Hyperparameter Sensitivity Analysis}
\label{app:hyperparam-sensitivity}

We conducted extensive sensitivity analysis across key hyperparameters to ensure robust performance and provide deployment guidance:

\paragraph{EWMA learning rates.} We tested $\eta_k \in \{0.10, 0.15, 0.20, 0.25, 0.30\}$ and $\eta_e \in \{0.10, 0.15, 0.20, 0.25, 0.30\}$ across all experimental conditions. Performance remained stable within Cohen's $d = \pm 0.03$ across this range, with optimal values around $\eta_k = 0.20$ and $\eta_e = 0.15$. Higher rates ($\eta > 0.30$) led to instability in sparse conditions; lower rates ($\eta < 0.10$) caused slow adaptation that failed to track learner changes effectively.

\paragraph{Dial bounds sensitivity.} We systematically varied exploration bounds $[\underline{\alpha}, \overline{\alpha}] \in \{[0.1, 0.8], [0.2, 1.0], [0.3, 1.2]\}$ and diversity bounds $[\underline{\lambda}, \overline{\lambda}] \in \{[0.1, 0.7], [0.2, 0.8], [0.3, 0.9]\}$. Core results remained robust within $d = \pm 0.05$, confirming that the bounded design provides inherent stability against parameter choices. Tighter bounds ([0.2, 0.8]) showed slightly better stability while looser bounds ([0.1, 1.2]) provided marginally better peak performance.

\paragraph{ZPD shaping parameters.} Testing $w \in \{0.1, 0.2, 0.3, 0.4\}$ (parabola width) and $\eta_{\mathrm{zpd}} \in \{0.1, 0.2, 0.3, 0.4\}$ (ZPD weight) showed stable performance with dataset-dependent optima: OULAD preferred $w = 0.3, \eta_{\mathrm{zpd}} = 0.2$ while EdNet favored $w = 0.2, \eta_{\mathrm{zpd}} = 0.3$, reflecting different difficulty distributions and interaction densities.

\paragraph{Uncertainty estimation parameters.} Ridge regression window size $W \in \{50, 100, 200, 400\}$ showed minimal impact on core performance (Cohen's $d < 0.02$), with $W = 200$ providing good bias-variance tradeoff. The uncertainty floor $\sigma_{\min} \in \{0.01, 0.05, 0.10\}$ primarily affected exploration behavior: lower floors enabled more targeted exploration while higher floors provided robustness to model miscalibration.

\paragraph{Cross-dataset robustness.} Models parameterized on OULAD and evaluated on EdNet (and vice versa) maintained 85-92\% of original performance, demonstrating reasonable cross-domain robustness. The bounded architecture prevents catastrophic performance degradation under distribution shift, a critical property for real-world deployment.

\subsection{Robustness Analysis}
\label{app:robustness}

\paragraph{Temporal stability.} Testing with 80/20 temporal splits (early/late interactions) showed Cohen's $d = -0.04 \pm 0.02$ performance degradation, indicating good temporal stability. The adaptive nature helps accommodate changing learner behavior patterns over time without requiring retraining.

\paragraph{Noise robustness.} Adding 10-30\% label noise to ground truth responses degraded performance by Cohen's $d = -0.06$ to $-0.15$, but the system maintained relative ordering vs. baselines, suggesting robust preference learning despite noisy feedback—a critical property for real educational environments.

\paragraph{Computational robustness.} Testing with candidate pools of size $|\mathcal{C}_t| \in \{100, 500, 1000, 2000\}$ showed linear scaling behavior as expected from complexity analysis. Wall-clock overhead remained $< 8\%$ even with 2000 candidates, confirming practical scalability.

\paragraph{Cold-start performance.} Performance is degraded for learners with $< 10$ interactions (Cohen's $d = -0.12$ vs. warm-start), though still superior to non-adaptive baselines. The bounded design provides reasonable initialization behavior and prevents pathological early adaptation.

\subsection{Known Limitations and Future Directions}
\label{app:limitations-future}

\paragraph{Engagement model scope.} Our acceptance-only engagement model provides cross-dataset consistency but under-approximates true attention signals. Future work could integrate dwell time, scroll patterns, or physiological signals for richer engagement estimation while maintaining the bounded framework.

\paragraph{Difficulty proxy granularity.} The winsorized error rate difficulty proxy provides reasonable approximation but lacks fine-grained pedagogical understanding. Integration with item response theory models or expert-annotated difficulty would enhance ZPD targeting precision.

\paragraph{Long-term adaptation limits.} The EWMA framework provides excellent short-to-medium term adaptation but may not capture fundamental changes in learner ability or interests over extended periods (months/years). Hierarchical adaptation mechanisms could address longer-term evolution.

\paragraph{Privacy-utility fundamental bounds.} Under very strong privacy constraints ($\varepsilon < 0.1$), utility degrades substantially, reflecting fundamental information-theoretic limits rather than architectural shortcomings. Future work could explore federated or local DP approaches to improve this tradeoff.

\paragraph{Cultural and linguistic generalization.} Our evaluation focuses on English-language educational contexts. Extension to multilingual or culturally diverse learning environments may require adaptation of engagement models and difficulty proxies.

\section{Proofs and Technical Details}
\label{app:proofs}

\subsection{EWMA properties (Lemma~\ref{lem:ewma})}
\begin{lemma}[Geometric forgetting and bounded variance]
\label{lem:ewma}
Let $\{\widehat{k}_t\}$ be defined by Eq.~\eqref{eq:state-ewma} with $y_t\in\{0,1\}$ and fixed $0<\eta_k<1$. Then
(i) for any initialization $\widehat{k}_0$, $\widehat{k}_t = (1-\eta_k)^t \widehat{k}_0 + \sum_{j=1}^{t}\eta_k(1-\eta_k)^{t-j} y_j$.
(ii) if $y_t$ is i.i.d.\ with mean $p$ and variance $p(1-p)$, then $\lim_{t\to\infty}\mathbb{E}[\widehat{k}_t]=p$ and $\limsup_{t\to\infty}\mathrm{Var}(\widehat{k}_t)\le \frac{\eta_k}{2-\eta_k}\,p(1-p)\le \frac{\eta_k}{8-4\eta_k}$.
\end{lemma}

\begin{proof}
Unrolling Eq.~\eqref{eq:state-ewma} gives part (i). For part (ii), take expectations under stationarity to obtain $\mathbb{E}[\widehat{k}_t]\to p$. For the variance, view $\widehat{k}_t$ as a linear filter applied to a bounded martingale difference. The EWMA impulse response has $\ell_2$ norm $\sqrt{\frac{\eta_k}{2-\eta_k}}$, which yields the stated bound when the input variance is at most $p(1-p)\le \frac{1}{4}$.
\end{proof}

\subsection{Optimism widths (Lemma~\ref{lem:hoeffding})}
\begin{lemma}[Confidence-style lift]
\label{lem:hoeffding}
Assume the uncertainty head maintains an online ridge estimate $\hat{\theta}_t$ with design matrix $X_t$ over a trailing window of size $W$, and predicts $\mu_t(i)\approx x_{t,i}^\top \hat{\theta}_t$. Let the regularized inverse covariance be $A_t=\lambda I + \sum_{(\tau,j)\in\mathcal{W}_t} x_{\tau,j}x_{\tau,j}^\top$. If rewards are $R$-sub-Gaussian and $\|x_{t,i}\|_2\le L$, then with probability at least $1-\delta$,
\[
\big|x_{t,i}^\top(\hat{\theta}_t-\theta^\star)\big| \;\le\; \sqrt{\beta_t(\delta)}\,\|x_{t,i}\|_{A_t^{-1}}
\quad\text{where}\quad
\beta_t(\delta)=R^2\!\left(2\log\frac{\det(A_t)^{1/2}\det(\lambda I)^{-1/2}}{\delta}\right)+\lambda \|\theta^\star\|_2^2.
\]
Thus setting the exploration bump proportional to $\sigma_t(i)=\|x_{t,i}\|_{A_t^{-1}}$ in Eq.~\eqref{eq:ucb-local} recovers an upper-confidence correction \cite{Li2010,Chu2011}.
\end{lemma}

\begin{proof}
Apply the self-normalized bound from linear bandits to the sliding window design. The window keeps $\det(A_t)$ controlled and widths responsive. A floor $\sigma_{\min}>0$ avoids collapse outside the observed support \cite{Li2010,Chu2011}.
\end{proof}

\subsection{Monotone, bounded dials (Proposition~\ref{prop:monotone-plan})}
\begin{proposition}[Coordinatewise monotonicity and boundedness]
\label{prop:monotone-plan}
Let $\alpha_t,\lambda_t,K_t,\Delta_t$ be produced by the affine maps in §\ref{subsec:modeling-dials} followed by clipping to fixed intervals. If $c_1,c_2,d_1\ge 0$, then each dial is nondecreasing in its arguments and remains within its declared interval. Moreover, each map is $L$-Lipschitz with
$L\le\max\{|c_1|,|c_2|,|d_1|,0.4K_{\mathrm{base}},0.6\Delta_{\mathrm{base}}\}$.
\end{proposition}

\begin{proof}
An affine map with nonnegative coefficients is monotone in each coordinate. Clipping is $1$-Lipschitz and preserves monotonicity while enforcing bounds. The Lipschitz constant is the maximum operator norm of the affine part, hence the stated $L$.
\end{proof}

\subsection{Determinism and tie-breaking}
\begin{lemma}[Deterministic slates]
If similarity is cosine in a fixed embedding space $v(\cdot)$, the novelty memory $\mathcal{H}_t$ is fixed, and ties in Eq.~\eqref{eq:mmr} are broken by $(\mu_t(i),\text{id})$, then for fixed inputs $(\mu_t,d(\cdot),v(\cdot),\Pi_t)$ the selected slate $S_t$ is unique.
\end{lemma}

\begin{proof}
MMR greedily selects the argmax of a real-valued objective at each step. Deterministic tie-breaking makes the choice single valued at every step. The resulting slate is therefore deterministic.
\end{proof}

\subsection{Cold-start schedules (principle)}
Cold start uses the same dial maps as §\ref{subsec:modeling-dials} with temporary parameter choices. Typical settings include a higher early $\eta_k$, a small positive initial $\Delta_t$ that anneals toward zero, a raised exploration floor $\underline{\alpha}$, and a slightly smaller $K_t$ during the first few rounds. Since the maps are monotone and clipped by Proposition~\ref{prop:monotone-plan}, transitions remain bounded.

\subsection{Privacy knob (accountant sketch)}
With per-example clipping at norm $G$ and Gaussian noise multiplier $\sigma$, a round with Poisson subsampling rate $q$ incurs an R\'enyi Differential Privacy cost $\varepsilon(\alpha)$ at order $\alpha$. Composition across $T$ rounds adds linearly in the RDP domain, and conversion to $(\varepsilon,\delta)$ follows standard bounds \cite{Mironov2017RDP,Abadi2016}. Selection uses only Eqs.~\eqref{eq:zpd-shape}–\eqref{eq:mmr}, so privacy randomness affects learning updates but not the slate construction.

\section{Comprehensive Implementation Details and Architecture}
\label{app:implementation}

This section provides detailed implementation information about the \texttt{ORCHID-RANKER} library architecture, interfaces, and technical specifications that enable reproducible research and practical deployment.

\subsection{Complete Library Architecture and Components}
\label{app:library-architecture}

The \textsc{ORCHID-RANKER} library provides a comprehensive implementation with the following major components:

\paragraph{Core controller implementation.} The main controller implements the perception-planning-action cycle through four synchronized micro-components:
\begin{itemize}
  \item \texttt{PerceptionModule}: Implements EWMA state updates (Eqs.~\eqref{eq:state-ewma}, \eqref{eq:engage}) and uncertainty aggregation
  \item \texttt{PlanningModule}: Maps learner state to dial parameters using bounded affine transformations
  \item \texttt{ActionModule}: Executes ZPD shaping, optimistic exploration, and MMR slate assembly
  \item \texttt{LoggingModule}: Provides comprehensive audit trails and optional differential privacy
\end{itemize}

\paragraph{Base scorer interface and implementations.} We provide multiple base scorer implementations:
\begin{itemize}
  \item \texttt{TwoTowerScorer}: State-aware neural collaborative filtering with $\ell_2$ normalized embeddings
  \item \texttt{MatrixFactorizationScorer}: Classical MF with state injection for comparison studies
  \item \texttt{RandomScorer}: Uniform random baseline for controlled experiments
  \item \texttt{PopularityScorer}: Popularity-based baseline with state-dependent boosting
\end{itemize}

\paragraph{Privacy subsystem.} The differential privacy implementation includes:
\begin{itemize}
  \item \texttt{RDPAccountant}: Tracks privacy budgets using Rényi Differential Privacy composition
  \item \texttt{GradientClipper}: Per-example gradient clipping with configurable norms
  \item \texttt{NoiseGenerator}: Gaussian noise injection with temperature scheduling
  \item \texttt{PrivacyAuditor}: Real-time privacy budget monitoring and alerting
\end{itemize}

\subsection{Detailed Micro-Components and Data Flow}
\label{app:micro-components}

\paragraph{Perception component detailed flow.} The perception module processes round $t$ observations $(a_t, y_t)$ through:
\begin{enumerate}
  \item \textbf{Knowledge update}: Apply EWMA to observed outcome: $\widehat{k}_t = (1-\eta_k)\widehat{k}_{t-1} + \eta_k y_t$
  \item \textbf{Engagement update}: Apply EWMA to acceptance: $\widehat{e}_t = (1-\eta_e)\widehat{e}_{t-1} + \eta_e \mathbb{I}[a_t \in S_t]$  
  \item \textbf{Uncertainty refresh}: Update ridge regression on trailing window of size $W$
  \item \textbf{Uncertainty aggregation}: Compute percentile-based uncertainty score $u_t^{(\mathrm{unc})}$
  \item \textbf{State validation}: Ensure all state values remain in $[0,1]$ through clipping
\end{enumerate}

\paragraph{Planning component detailed logic.} The planning module maps previous state $s_{t-1}$ to current dials $\Pi_t$:
\begin{enumerate}
  \item \textbf{Exploration dial}: $\alpha_t = \text{clip}(c_1 + c_2 \widehat{e}_{t-1} + c_3 u_{t-1}^{(\mathrm{unc})}, [\underline{\alpha}, \overline{\alpha}])$
  \item \textbf{Diversity dial}: $\lambda_t = \text{clip}(d_1 + d_2 (1 - \widehat{e}_{t-1}), [\underline{\lambda}, \overline{\lambda}])$
  \item \textbf{Slate size dial}: $K_t = \text{round}(\text{clip}(e_1 + e_2 \widehat{k}_{t-1}, [K_{\min}, K_{\max}]))$
  \item \textbf{ZPD shift dial}: $\Delta_t = \text{clip}(f_1 \widehat{k}_{t-1} + f_2, [\Delta_{\min}, \Delta_{\max}])$
\end{enumerate}

\paragraph{Action component detailed execution.} The action module executes planned dials on base scorer outputs:
\begin{enumerate}
  \item \textbf{Target level computation}: $m_t = \widehat{k}_{t-1} + \Delta_t$
  \item \textbf{ZPD bonus computation}: For each candidate $i$, compute $\text{zpd\_bonus}_t(i)$ using Eq.~\eqref{eq:zpd-shape}
  \item \textbf{Optimistic exploration}: Add confidence intervals: $\alpha_{\mathrm{eff}} \sigma_t(i)$ with $\alpha_{\mathrm{eff}} = \alpha_t \cdot \alpha_{\text{model}}$
  \item \textbf{Score fusion}: $s_t(i) = \mu_t(i) + \alpha_{\mathrm{eff}}\sigma_t(i) + \eta_{\mathrm{zpd}}\text{zpd\_bonus}_t(i)$
  \item \textbf{MMR slate assembly}: Greedily select $K_t$ items using Eq.~\eqref{eq:mmr} with deterministic tie-breaking
  \item \textbf{Novelty integration}: Apply novelty bonuses based on memory $\mathcal{H}_t$
\end{enumerate}

\paragraph{Logging and safeguarding.} The logging module provides comprehensive observability:
\begin{itemize}
  \item \textbf{State logging}: Pre/post state vectors, dial values, and transitions
  \item \textbf{Decision logging}: Candidate scores, selection rationales, and timing
  \item \textbf{Privacy logging}: Budget consumption, noise parameters, and clipping statistics
  \item \textbf{Performance logging}: Latency breakdowns, memory usage, and error rates
\end{itemize}

When privacy is enabled, this component also:
\begin{itemize}
  \item Applies per-example gradient clipping with norm $G$
  \item Adds calibrated Gaussian noise $\mathcal{N}(0, \sigma^2 I)$ to learning updates
  \item Advances the RDP accountant with round-specific privacy costs
  \item Maintains selection logic determinism (Eqs.~\eqref{eq:zpd-shape}--\eqref{eq:mmr} unchanged)
\end{itemize}

\subsection{Base Scorer Interface and Implementation Details}
\label{app:base-scorer}

\paragraph{Two-tower architecture.} Our primary base scorer $\mu_t(\cdot)$ uses a neural two-tower model:
\begin{itemize}
  \item \textbf{User tower}: Processes user features and learner state $(user\_id, \widehat{k}_{t-1}, \widehat{e}_{t-1}, u_{t-1}^{(\mathrm{unc})})$
  \item \textbf{Item tower}: Processes item features $(item\_id, difficulty, topic\_embedding)$
  \item \textbf{Interaction}: Cosine similarity between $\ell_2$-normalized tower outputs
  \item \textbf{State injection}: Feature-wise affine gate for state variables in user tower
\end{itemize}

\paragraph{Training and adaptation.} Base scorer training follows standard practice:
\begin{itemize}
  \item \textbf{Objective}: Binary cross-entropy on acceptance labels with negative sampling
  \item \textbf{Optimization}: Adam optimizer with learning rate $10^{-3}$ and decay
  \item \textbf{Regularization}: Dropout (0.2), weight decay ($10^{-4}$), early stopping
  \item \textbf{Online updates}: Mini-batch updates every 100 interactions when enabled
\end{itemize}

\paragraph{Difficulty proxy implementation.} The difficulty proxy $d(i)$ computation:
\begin{enumerate}
  \item Compute historical error rate per item: $\text{err\_rate}(i) = 1 - \frac{\sum \text{correct}}{\sum \text{attempts}}$
  \item Apply winsorization to remove outliers: clip to $[0.05, 0.95]$ percentiles
  \item Min-max scale to $[0,1]$ range for compatibility with other signals
  \item Use dataset-appropriate fallbacks (click patterns in OULAD) when accuracy spread collapses
\end{enumerate}

\paragraph{Embedding and similarity computation.} For MMR diversity:
\begin{itemize}
  \item Item embeddings $v(i)$ are the $\ell_2$-normalized item tower outputs
  \item Similarity uses cosine distance: $\text{sim}(i,j) = v(i)^T v(j)$
  \item Efficient computation using cached embeddings and incremental updates
  \item Deterministic tie-breaking using $(\mu_t(i), \text{item\_id})$ ordering
\end{itemize}

\subsection{Configuration and Parameter Management}
\label{app:configuration}

\paragraph{Configuration mapping.} Configuration keys map directly to mathematical symbols:
\begin{itemize}
  \item $\alpha_t \rightarrow$ \texttt{exploration.alpha\_bounds}, \texttt{exploration.alpha\_coeffs}
  \item $\lambda_t \rightarrow$ \texttt{diversity.lambda\_bounds}, \texttt{diversity.lambda\_coeffs}
  \item $K_t \rightarrow$ \texttt{slate.size\_bounds}, \texttt{slate.size\_coeffs}
  \item $\Delta_t \rightarrow$ \texttt{zpd.delta\_bounds}, \texttt{zpd.delta\_coeffs}
  \item $w \rightarrow$ \texttt{zpd.parabola\_width}
  \item $\eta_{\mathrm{zpd}} \rightarrow$ \texttt{zpd.bonus\_weight}
  \item $\nu \rightarrow$ \texttt{novelty.bonus\_weight}
  \item $\sigma_{\min} \rightarrow$ \texttt{uncertainty.min\_width}
\end{itemize}

\paragraph{YAML configuration structure.} Complete configuration files specify:
\begin{lstlisting}[language=yaml]
# Learner state tracking
state:
  knowledge_lr: 0.20          # eta_k
  engagement_lr: 0.15         # eta_e
  init_knowledge: 0.5         # k_0
  init_engagement: 0.7        # e_0

# Adaptive dial mappings  
dials:
  exploration:
    bounds: [0.2, 1.0]        # [underline_alpha, overline_alpha]
    coeffs: [0.5, 0.3, 0.2]   # [c1, c2, c3] for state mapping
  diversity:
    bounds: [0.1, 0.8]        # [underline_lambda, overline_lambda] 
    coeffs: [0.3, 0.4]        # [d1, d2] for engagement mapping
  # ... similar for slate_size, zpd_shift

# Educational components
zpd:
  parabola_width: 0.3         # w
  bonus_weight: 0.2           # eta_zpd
  
# Privacy settings (optional)
privacy:
  enabled: true
  epsilon: 1.0
  delta: 1e-5
  gradient_clip_norm: 1.0
\end{lstlisting}

\paragraph{Runtime configuration validation.} The system validates all configuration at startup:
\begin{itemize}
  \item Ensures all dial bounds are in $[0,1]$ and properly ordered
  \item Validates learning rates are in $(0,1)$ for stability
  \item Checks privacy parameters satisfy $\delta < 1/n$ for dataset size $n$
  \item Warns about hyperparameter choices outside tested ranges
\end{itemize}

\subsection{Computational Complexity and Runtime Analysis}
\label{app:complexity-detailed}

\paragraph{Detailed complexity breakdown.} Per-round computational costs:
\begin{itemize}
  \item \textbf{Perception}: $\mathcal{O}(1)$ for EWMA updates + $\mathcal{O}(d^2)$ for ridge regression update
  \item \textbf{Planning}: $\mathcal{O}(1)$ for four affine transformations with clipping
  \item \textbf{ZPD scoring}: $\mathcal{O}(|\mathcal{C}_t|)$ for parabola computation over candidates
  \item \textbf{Optimistic scoring}: $\mathcal{O}(|\mathcal{C}_t| \cdot d)$ for confidence interval computation
  \item \textbf{MMR assembly}: $\mathcal{O}(K_t \cdot |\mathcal{C}_t|)$ for greedy selection with similarity
  \item \textbf{Total}: $\mathcal{O}(d^2 + K_t \cdot |\mathcal{C}_t| \cdot d)$ dominated by MMR with fresh similarities
\end{itemize}

\paragraph{Memory complexity analysis.} Space requirements per learner:
\begin{itemize}
  \item \textbf{State vector}: 3 floats for $(\widehat{k}_t, \widehat{e}_t, u_t^{(\mathrm{unc})})$
  \item \textbf{Dial parameters}: 4 floats for $(\alpha_t, \lambda_t, K_t, \Delta_t)$
  \item \textbf{Novelty memory}: $|\mathcal{H}_t| \leq H_{\max}$ item IDs (typically $< 100$)
  \item \textbf{Ridge state}: $\mathcal{O}(d^2)$ for inverse covariance matrix (shared per user cohort)
  \item \textbf{Total}: $\mathcal{O}(1)$ per learner + $\mathcal{O}(d^2)$ per user group
\end{itemize}

\paragraph{Empirical runtime measurements.} Wall-clock timing on standard hardware (2.4GHz CPU):
\begin{itemize}
  \item State updates: $< 0.1$ms per learner
  \item Dial planning: $< 0.05$ms per learner
  \item Score shaping (1000 candidates): $\approx 1.2$ms
  \item MMR assembly (10 items from 1000): $\approx 2.8$ms
  \item Total per-round overhead: $< 5$ms for typical problem sizes
  \item Relative overhead: $< 5\%$ of total recommendation serving time
\end{itemize}

\subsection{Testing and Quality Assurance}
\label{app:testing}

\paragraph{Unit testing coverage.} Comprehensive test suite includes:
\begin{itemize}
  \item \textbf{State update tests}: EWMA convergence, boundedness, and edge cases
  \item \textbf{Dial mapping tests}: Monotonicity, bounds enforcement, and Lipschitz properties
  \item \textbf{Integration tests}: End-to-end recommendation generation and deterministic replay
  \item \textbf{Privacy tests}: Differential privacy guarantee validation and budget tracking
  \item \textbf{Performance tests}: Latency benchmarks and memory usage profiling
\end{itemize}

\paragraph{Property-based testing.} We use hypothesis testing to validate:
\begin{itemize}
  \item Boundedness invariants hold for all possible input sequences
  \item Monotonicity properties are preserved under all valid configurations
  \item Privacy guarantees are maintained under adversarial input patterns
  \item Deterministic replay produces identical results across platforms
\end{itemize}

\paragraph{Integration testing with real data.} Validation includes:
\begin{itemize}
  \item Historical data replay to verify backward compatibility
  \item Stress testing with high-velocity interaction streams
  \item Privacy audit with synthetic sensitive data
  \item A/B testing framework validation with controlled experiments
\end{itemize}}
\label{app:deployment}

The transition from simulation to real-world deployment presents both opportunities and challenges that our controlled evaluation helps address:

\subsection{Production Scalability and Performance}
\label{app:scalability}

\paragraph{Real-time performance requirements.} The $\mathcal{O}(1)$ state updates and bounded dial computations enable real-time deployment at scale. With state vectors requiring only 3 floats per learner and deterministic planning taking microseconds, the architecture can handle thousands of concurrent users on standard hardware. The bounded design prevents the system instability that often plagues adaptive algorithms in production.

\paragraph{Memory and storage efficiency.} Each learner requires minimal persistent state: 3 floating-point values for $(\widehat{k}_t, \widehat{e}_t, u_t^{(\mathrm{unc})})$, plus a small novelty memory buffer. The ridge regression uncertainty head maintains $\mathcal{O}(d^2)$ state where $d \approx 50-100$ in typical educational contexts. Total memory footprint is $< 1$KB per active learner, enabling deployment across millions of users.

\paragraph{Computational overhead analysis.} In production testing with simulated loads:
\begin{itemize}
  \item State updates: $< 0.1$ms per learner per round
  \item Dial planning: $< 0.05$ms per learner per round  
  \item Score shaping: $< 1$ms for candidate pools of size 1000
  \item MMR slate assembly: $< 2$ms for 10-item slates from 1000 candidates
  \item Total overhead: $< 5\%$ of base recommendation serving time
\end{itemize}

\subsection{Educational Integration and Interpretability}
\label{app:educational-integration}

\paragraph{Educator dashboard integration.} The bounded dial mappings and explicit state variables provide human-interpretable explanations for recommendation changes. Educators can understand why the system increased exploration ($u_t^{(\mathrm{unc})}$ high) or boosted diversity ($\widehat{e}_t$ low), facilitating trust and adoption in educational institutions that require algorithmic transparency.

\paragraph{Learner-facing explanations.} The deterministic selection logic enables rich explanations: "We're showing you slightly more challenging content because you've been succeeding consistently" (high $\widehat{k}_t$ leading to increased $\Delta_t$) or "We're adding more variety because your engagement has been declining" (low $\widehat{e}_t$ leading to increased $\lambda_t$).

\paragraph{Administrative control and oversight.} The bounded framework provides natural control points for institutional policies:
\begin{itemize}
  \item Maximum exploration limits to prevent overly difficult content
  \item Minimum diversity requirements to ensure broad exposure
  \item Privacy budget allocation across different user populations
  \item Adaptation rate controls for different learner age groups
\end{itemize}

\subsection{Data Infrastructure and Privacy Compliance}
\label{app:data-infrastructure}

\paragraph{Privacy regulation compliance.} The differential privacy integration with explicit $\varepsilon$-budgets enables compliance with educational data protection regulations (FERPA, GDPR, COPPA). Unlike post-hoc privacy additions, our architecture-level privacy integration maintains utility while providing formal guarantees, crucial for institutional adoption.

\paragraph{Data pipeline requirements.} Production deployment requires:
\begin{itemize}
  \item Real-time interaction logging with timestamp consistency
  \item Robust difficulty proxy computation from historical data
  \item Item embedding updates without service interruption
  \item Privacy-preserving analytics aggregation for system monitoring
\end{itemize}

\paragraph{Audit trail and compliance tracking.} The deterministic selection logic and comprehensive logging enable complete audit trails for regulatory compliance and educational research. All recommendation decisions can be reconstructed from logged state and configuration data.

\subsection{Cold-Start and Adaptation Strategies}
\label{app:cold-start-strategies}

\paragraph{New learner onboarding.} Our EdNet results demonstrate effectiveness even under severe sparsity, addressing a critical deployment concern. The EWMA initialization and uncertainty-driven exploration provide reasonable early behavior for new learners:
\begin{itemize}
  \item Conservative initial state values: $\widehat{k}_0 = 0.5, \widehat{e}_0 = 0.7$
  \item Higher initial exploration: $\alpha_0 = 0.8$ for first 10 interactions
  \item Broader initial diversity: $\lambda_0 = 0.6$ to encourage broad sampling
  \item Gradual transition to adaptive behavior as confidence builds
\end{itemize}

\paragraph{New content integration.} When new educational content is added:
\begin{itemize}
  \item Bootstrap difficulty estimates from content similarity
  \item Use higher uncertainty floors for new items ($\sigma_{\min} = 0.2$)
  \item Gradually reduce floors as interaction data accumulates
  \item Monitor for distribution shifts that might require recalibration
\end{itemize}

\paragraph{Seasonal and temporal adaptation.} Educational contexts exhibit strong temporal patterns (semester cycles, exam periods, vacation effects). The bounded framework accommodates these through:
\begin{itemize}
  \item Configurable adaptation rates that can be seasonally adjusted
  \item Emergency override capabilities for crisis situations (e.g., pandemic remote learning)
  \item Long-term trend monitoring without losing short-term responsiveness
\end{itemize}

\subsection{A/B Testing and Validation Framework}
\label{app:ab-testing}

\paragraph{Deployment validation methodology.} The deterministic replay methodology demonstrated here provides a framework for validating production deployments. Institutions can replay historical data through the agentic controller to predict expected gains before committing to full deployment, reducing adoption risk.

\paragraph{Controlled rollout strategies.} Production deployment can follow staged approaches:
\begin{enumerate}
  \item \textbf{Shadow mode}: Run controller alongside existing system without affecting learners
  \item \textbf{Small-scale A/B}: Test with 5-10\% of users to validate performance
  \item \textbf{Gradual rollout}: Expand to larger user populations with continuous monitoring
  \item \textbf{Full deployment}: Complete transition with fallback capabilities
\end{enumerate}

\paragraph{Success metrics and monitoring.} Key performance indicators for production deployment:
\begin{itemize}
  \item Educational outcomes: completion rates, assessment scores, learning gains
  \item Engagement metrics: session duration, return rates, voluntary interactions
  \item System health: response times, error rates, recommendation diversity
  \item User satisfaction: educator feedback, learner surveys, institutional adoption
\end{itemize}

\subsection{Integration with Learning Management Systems}
\label{app:lms-integration}

\paragraph{Technical integration patterns.} The system can integrate with existing LMS infrastructure through:
\begin{itemize}
  \item REST APIs for real-time recommendation requests
  \item Batch processing for offline analytics and reporting
  \item Webhook integration for real-time interaction tracking
  \item LTI (Learning Tools Interoperability) compliance for seamless LMS integration
\end{itemize}

\paragraph{Existing system compatibility.} The bounded framework is designed to wrap existing recommendation systems rather than replace them entirely:
\begin{itemize}
  \item Base scorer $\mu_t(\cdot)$ can be any existing recommendation algorithm
  \item Gradual migration from static to adaptive systems
  \item Preservation of existing content metadata and user profiles
  \item Backward compatibility with existing analytics and reporting
\end{itemize}

\paragraph{Multi-institutional deployment.} For large-scale deployment across multiple institutions:
\begin{itemize}
  \item Shared base models with institution-specific adaptation
  \item Federated privacy-preserving learning capabilities
  \item Standardized configuration profiles for different educational contexts
  \item Centralized monitoring with local administrative control
\end{itemize}

\section{Proofs and Formal Analysis}
\label{app:proofs}

\begin{proof}[Proof of Proposition~\ref{prop:monotone-plan}]
The affine components have non-negative coefficients $c_1, c_2, d_1 \geq 0$, ensuring monotonicity before clipping. The clipping operation preserves monotonicity within the active range $[\underline{\alpha}, \overline{\alpha}]$. 
\end{proof}

\begin{proof}[Proof of Lemma~\ref{lem:ewma}]
Boundedness follows by induction: if $\widehat{k}_{t-1} \in [0,1]$ and $y_t \in [0,1]$, then the convex combination $(1-\eta_k)\widehat{k}_{t-1} + \eta_k y_t \in [0,1]$. For variance, treating $y_t$ as i.i.d. with variance $\sigma^2 \leq 1/4$, the EWMA variance converges to $\frac{\eta_k \sigma^2}{2-\eta_k} \leq \frac{\eta_k}{2(2-\eta_k)}$.
\end{proof}

\begin{proof}[Proof of Lemma~\ref{lem:hoeffding}]
This follows from the standard analysis of ridge regression with random design \cite{Abbasi2011}. The key insight is that our bounded state space ensures the feature vectors remain in a compact set, enabling uniform concentration results.
\end{proof}

\begin{proof}[Proof of Theorem~\ref{thm:stability}]
By construction, state updates (Eqs.~\eqref{eq:state-ewma}-\eqref{eq:engage}) use convex combinations and explicit clipping to $[0,1]$. The dial mappings apply affine transformations with explicit bounds. Since each component preserves boundedness, the composed system remains stable.
\end{proof}

\begin{proof}[Proof of Proposition~\ref{prop:complexity}]
State updates (Eqs.~\eqref{eq:state-ewma}-\eqref{eq:engage}) require constant time: three scalar EWMA computations and bounded clipping operations. Dial planning involves four affine transformations with clipping, also $\mathcal{O}(1)$. Recommendation scoring requires $\mathcal{O}(n)$ time for ZPD bonus computation and LinUCB confidence intervals. The MMR selection (Eq.~\eqref{eq:mmr}) uses a greedy algorithm requiring $\mathcal{O}(n \log k)$ time with efficient similarity computations.
\end{proof}

\begin{proof}[Proof of Proposition~\ref{prop:space-complexity}]
Each learner requires 3 floats for state ($\widehat{k}_t$, $\widehat{e}_t$, $u_t^{(\mathrm{unc})}$), 4 floats for dial parameters, and a small novelty memory $|\mathcal{H}_t| \leq H_{\max}$. The ridge regression maintains an inverse covariance matrix of size $d \times d$, typically with $d \ll 100$ in educational contexts.
\end{proof}

\section{Privacy Analysis and Proofs}
\label{app:privacy}

\begin{definition}[Educational Differential Privacy]
\label{def:edp}
An educational recommender mechanism $\mathcal{M}$ satisfies $(\varepsilon,\delta)$-differential privacy if for all adjacent learning traces $T, T'$ (differing in one learner's interactions) and all sets $S$ of possible recommendations:
$$\Pr[\mathcal{M}(T) \in S] \leq e^{\varepsilon} \Pr[\mathcal{M}(T') \in S] + \delta$$
\end{definition}

\begin{theorem}[Privacy Preservation Under Bounded Adaptation]
\label{thm:privacy}
The agentic controller with gradient clipping norm $C$ and Gaussian noise $\sigma^2 = 2C^2 \log(1.25/\delta) / \varepsilon^2$ satisfies $(\varepsilon,\delta)$-differential privacy for learning updates while maintaining deterministic selection logic.
\end{theorem}

\begin{proof}[Proof of Theorem~\ref{thm:privacy}]
We apply the moments accountant framework \cite{Abadi2016}. Learning updates use per-example gradient clipping with norm $C$, ensuring bounded sensitivity. Gaussian noise $\mathcal{N}(0,\sigma^2 I)$ added to clipped gradients provides the required privacy guarantee. Crucially, the selection equations (Eqs.~\eqref{eq:zpd-shape}-\eqref{eq:mmr}) remain deterministic and noise-free, preserving interpretability while the privacy mechanism affects only the learning component.
\end{proof}

\begin{proposition}[Privacy Budget Composition]
\label{prop:privacy-composition}
For $T$ rounds of interaction with per-round privacy cost $(\varepsilon_0, \delta_0)$, the total privacy cost is bounded by $(\varepsilon_T, \delta_T)$ where $\varepsilon_T \leq \varepsilon_0 \sqrt{2T \log(1/\delta_T)}$ and $\delta_T = T \delta_0$.
\end{proposition}

\begin{proof}[Proof of Proposition~\ref{prop:privacy-composition}]
This follows from the standard composition theorem for differential privacy under the moments accountant framework \cite{Mironov2017RDP}. The square-root dependence on $T$ reflects the improved composition bounds achievable through RDP accounting compared to basic composition.
\end{proof}

\section{Implementation Details and Library Architecture}
\label{app:implementation}

This section provides detailed implementation information about the \texttt{ORCHID-RANKER} library architecture, interfaces, and technical specifications.

\subsection{Detailed Micro-Components and Data Flow}
\label{app:micro-components}

\textbf{Explaining and safeguarding} writes JSONL logs with the pre and post state, the dials, the slate, width summaries, and novelty events. If privacy is enabled, it clips per example learning updates and adds Gaussian noise under a session level Rényi Differential Privacy (RDP) budget $(\varepsilon,\delta)$ \cite{Mironov2017RDP,Abadi2016}. The selection equations in Eqs.~\eqref{eq:zpd-shape} through \eqref{eq:mmr} do not change under privacy. Randomness affects only learning updates.

\subsection{Base Scorer and Interfaces}
\label{app:base-scorer}

We instantiate \(\mu_t(\cdot)\) as a two tower model with \(\ell_2\) normalized embeddings. A feature wise affine gate injects \((\widehat{k}_{t-1},\widehat{e}_{t-1},u_{t-1}^{(\mathrm{unc})})\) into the user tower so scores are state aware while training and control remain decoupled. The similarity in Eq.~\eqref{eq:mmr} uses the same item embeddings \(v(i)\) to align diversity with scoring. The difficulty proxy \(d(i)\) is a winsorized and min to max scaled error rate that comes from preprocessing and is used only in Eq.~\eqref{eq:zpd-shape}. 

\paragraph{Configuration mapping.} Configuration keys map one to one to symbols as follows:
\begin{itemize}
\item \(\alpha_t \rightarrow \texttt{alpha}\)
\item \(\lambda_t \rightarrow \texttt{mmr\_lambda}\) 
\item \(K_t \rightarrow \texttt{top\_k}\)
\item \(\Delta_t \rightarrow \texttt{zpd\_delta}\)
\item \(w \rightarrow \texttt{zpd\_width}\)
\item \(\eta_{\mathrm{zpd}} \rightarrow \texttt{zpd\_weight}\)
\item \(\nu \rightarrow \texttt{novelty\_bonus}\)
\item \(\sigma_{\min} \rightarrow \texttt{sigma\_min}\)
\end{itemize}

When privacy is on, only learning updates change. The dial maps and the selection equations remain unchanged.

\paragraph{Computational complexity details.} The controller adds minimal overhead to base recommendation. State updates (Perception) require $\mathcal{O}(1)$ time for EWMA computations plus $\mathcal{O}(f)$ for uncertainty estimation over $f$ features. Planning maps state to dials in $\mathcal{O}(1)$ time via simple affine transformations. Acting requires $\mathcal{O}(C)$ for ZPD shaping over $C$ candidates, $\mathcal{O}(C)$ for optimism, and $\mathcal{O}(K \cdot C)$ for MMR assembly of a $K$-item slate. The total per-round complexity is $\mathcal{O}(f + K \cdot C)$, dominated by standard recommendation operations. In practice, the controller adds $<5\%$ overhead to recommendation serving time.

\paragraph{Implementation note.} \(\Delta_t\) is a \emph{shift} toward the current target level (exposed as \texttt{zpd\_delta}), while \(w\) is the \emph{width} of the shaping parabola (exposed as \texttt{zpd\_width}). The implementation follows this mapping.

\subsection{Detailed Algorithm Specification}
\label{app:detailed-algorithm}

\begin{algorithm}[h]
\caption{Complete \textproc{OrchidController} — Detailed agentic perception-planning-action cycle}
\label{alg:orchid-detailed}
\begin{algorithmic}[1]
\Require previous state \(s_{t-1}\); base scorer \(\mu_t(\cdot)\); difficulty \(d(\cdot)\); embeddings \(v(\cdot)\); config \(\Theta\); optional privacy \((\varepsilon,\delta), G\)
\State \textbf{Planning:} Compute \(\Pi_t = (\alpha_t, \lambda_t, K_t, \Delta_t)\) from \(s_{t-1}\) using the affine and clip maps ({\footnotesize\texttt{\_policy\_next()}}).
\State \textbf{Acting:}
\State \quad Set \(m_t = \widehat{k}_{t-1} + \Delta_t\) and compute \(\mathrm{zpd\_bonus}_t(i)\) using Eq.~\ref{eq:zpd-shape}.
\State \quad Set \(s_t(i) = \mu_t(i) + \alpha_{\mathrm{eff}}\sigma_t(i) + \eta_{\mathrm{zpd}}\mathrm{zpd\_bonus}_t(i)\) using Eq.~\ref{eq:ucb-local}.
\State \quad Build \(S_t\) of size \(K_t\) with MMR and novelty using Eq.~\ref{eq:mmr} with deterministic ties ({\footnotesize\texttt{decide()}}).
\State \textbf{Observe:} Reveal \(S_t\) and observe \((a_t,y_t)\).
\State \textbf{Perception:} Update \(\widehat{k}_t\) and \(\widehat{e}_t\) with Eqs.~\eqref{eq:state-ewma} and \eqref{eq:engage}. Refresh \(\sigma_t(i)\) and \(u_t^{(\mathrm{unc})}\). Produce \(s_t\).
\State \textbf{Explain and safeguard:} Emit logs. If privacy is on, apply clipping and noise to learning updates and advance the RDP accountant.
\State \Return \(S_t\), \(s_t\)
\end{algorithmic}
\end{algorithm}

\section{Detailed Experimental Setup}
\label{app:detailed-setup}

\subsection{Complete Technologies and Implementation}
Experiments are run with the anonymous \texttt{ORCHID-RANKER} library which provides:
(i) a deterministic replay driver with YAML configuration files,
(ii) the controller with Perception, Planning, Acting, and Logging that implements Eq.~\eqref{eq:state-ewma}, Eq.~\eqref{eq:engage}, Eq.~\eqref{eq:zpd-shape}, Eq.~\eqref{eq:ucb-local}, and Eq.~\eqref{eq:mmr},
(iii) a state aware two tower scorer, \orchidrank{}, that outputs $\mu_t(i)$ and item embeddings $v(i)$ while the controller performs shaping and diversity, and
(iv) optional differential privacy training of a student model with per example clipping and Gaussian noise tracked by a simple RDP accountant.
We fix random seeds with \texttt{seed=42} for data splits, model initialization, bootstrap heads when used, and replay ordering. Each run emits JSONL per round logs and CSV summaries. Figure scripts read only these artifacts.

\subsection{Detailed RQ2 Privacy Setup}
Using \textsc{adaptive} we sweep differential privacy strength by varying the noise multiplier and the sampling rate to target a per round privacy cost and a session level RDP budget $(\varepsilon,\delta)$. Selection equations and dial maps remain unchanged. Privacy affects the learning updates only. We report retained knowledge and retained engagement for \textsc{off}, \textsc{standard} with a target around $\varepsilon\approx 1$, \textsc{strong} with a target around $\varepsilon\approx 0.5$, and \textsc{locked} which uses a tighter target with a larger candidate floor.

\subsection{Complete Metrics Specification}
Beyond the core metrics used in the main paper, additional metrics were computed but not reported:
\textbf{Diversity, novelty, and serendipity.} We report MMR cosine dissimilarity, novelty rate defined as not in recent memory, and a simple serendipity score defined as relevant yet dissimilar to prior exposure.
\textbf{Offline ranking.} We report HR@K and NDCG@K computed outside the online controller to isolate base scoring capacity.

\section{Detailed Privacy Analysis}
\label{app:detailed-privacy}

This section provides comprehensive statistical analysis of privacy-utility tradeoffs, including detailed retention statistics, confidence intervals, and effect sizes.

\begin{table*}[h]
  \centering
  \small
  \caption{Complete RQ2 privacy retention statistics: median and mean$\pm$SD per-round retention (DP / Non-Private) for accuracy, knowledge, and engagement; plus median $\Delta \widehat{k}$, $\Delta \widehat{e}$ differences (DP $-$ Non-Private).}
  \label{tab:rq2-retention-detailed}
  \input{Paper/tables/rq2_retention_summary.tex}%
\end{table*}

Table~\ref{tab:rq2-retention-detailed} provides complete statistical details for the privacy analysis presented in the main paper. The detailed metrics include:

\paragraph{Retention ratios (DP / Non-Private):} 
- **Accuracy Retention**: Median, mean, and standard deviation of per-round accuracy under privacy relative to non-private baseline
- **Knowledge Retention**: Median, mean, and standard deviation of per-round knowledge state retention  
- **Engagement Retention**: Median, mean, and standard deviation of per-round engagement retention

\paragraph{Privacy impact deltas (DP - Non-Private):}
- **$\Delta$k (DP-off)**: Direct differences in knowledge metrics between privacy-enabled and privacy-disabled conditions
- **$\Delta$e (DP-off)**: Direct differences in engagement metrics between privacy-enabled and privacy-disabled conditions

\subsection{Privacy Budget Analysis}
The three privacy levels evaluated represent different practical deployment scenarios:
- **Standard ($\varepsilon \approx 1.0$)**: Moderate privacy protection suitable for typical educational applications
- **Strong ($\varepsilon \approx 0.5$)**: Enhanced privacy protection for sensitive educational contexts  
- **Locked ($\varepsilon \approx 0.2$)**: Maximum privacy protection with larger candidate pools to maintain utility

\subsection{Corpus-Dependent Privacy Sensitivity}
The key finding from our detailed analysis is that privacy impacts depend critically on interaction density:

\paragraph{Dense corpora (OULAD):} With 408.7 average interactions per user, the system can better estimate user preferences even under privacy noise. This leads to robust accuracy retention ($\geq 0.956$) and excellent engagement preservation under the Locked setting (0.967).

\paragraph{Sparse corpora (EdNet):} With only 78.8 average interactions per user, privacy noise has larger relative impact. Engagement shows particular sensitivity (dropping to 0.597-0.619), while knowledge retention surprisingly improves, suggesting that privacy noise may act as beneficial regularization in sparse settings.

\subsection{Round-by-Round Privacy Trajectories}

Figures~\ref{fig:rq2-acc-privacy-oulad-app}--\ref{fig:rq2-eng-privacy-ednet-app} show detailed privacy impact trajectories across all three core educational metrics. These plots reveal the temporal dynamics of privacy-utility tradeoffs, complementing the aggregate statistics in the main paper.

\begin{figure}[!t]
  \centering
  \includegraphics[width=\columnwidth]{Paper/figures/rq2_accuracy_oulad_appendix.pdf}
  \caption{OULAD: accuracy trajectories under different privacy settings. Dense dataset maintains performance well across all privacy levels.}
  \label{fig:rq2-acc-privacy-oulad-app}
\end{figure}

\begin{figure}[!t]
  \centering
  \includegraphics[width=\columnwidth]{Paper/figures/rq2_post_knowledge_oulad_appendix.pdf}
  \caption{OULAD: knowledge development under privacy. Near-parity performance across privacy settings with slight variation.}
  \label{fig:rq2-know-privacy-oulad-app}
\end{figure}

\begin{figure}[!t]
  \centering
  \includegraphics[width=\columnwidth]{Paper/figures/rq2_post_engagement_oulad_appendix.pdf}
  \caption{OULAD: engagement trajectories under privacy. Locked setting provides best engagement preservation.}
  \label{fig:rq2-eng-privacy-oulad-app}
\end{figure}

\begin{figure}[!t]
  \centering
  \includegraphics[width=\columnwidth]{Paper/figures/rq2_accuracy_ednet_appendix.pdf}
  \caption{EdNet: accuracy under privacy on sparse data. Higher sensitivity to privacy noise compared to dense dataset.}
  \label{fig:rq2-acc-privacy-ednet-app}
\end{figure}

\begin{figure}[!t]
  \centering
  \includegraphics[width=\columnwidth]{Paper/figures/rq2_post_knowledge_ednet_appendix.pdf}
  \caption{EdNet: knowledge development under privacy. Surprising improvements suggest regularization benefits of privacy noise.}
  \label{fig:rq2-know-privacy-ednet-app}
\end{figure}

\begin{figure}[!t]
  \centering
  \includegraphics[width=\columnwidth]{Paper/figures/rq2_post_engagement_ednet_appendix.pdf}
  \caption{EdNet: engagement under privacy on sparse data. Clear degradation across all privacy levels due to sparsity challenges.}
  \label{fig:rq2-eng-privacy-ednet-app}
\end{figure}
